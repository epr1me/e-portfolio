<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Unit 8 - Gradient Cost Function</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../../index.html" class="title">Paul Dogar</a>
				<nav>
					<ul>
						<li><a href="/e-portfolio/index.html">Back Home</a></li>
						<li><a href="/e-portfolio/modules/machine-learning.html">Machine Learning</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="main" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 8 - Gradient Cost Function</h1>
							<p>This unit focused on understanding gradient descent for optimizing linear regression models. Experimentation with iteration numbers and learning rates provided insights into cost function convergence and model optimization.</p>

							<h2>Key Learning Outcomes</h2>
							<ul>
								<li><strong>Gradient Descent Dynamics:</strong> Observed how increasing iterations reduced cost, improving model accuracy, and how learning rates impacted convergence speed and stability.</li>
								<li><strong>Applicability and Challenges:</strong> Explored hyperparameter tuning for achieving balance between speed and accuracy, and discussed generalization challenges for different datasets.</li>
								<li><strong>Ethical Considerations:</strong> Highlighted the importance of ensuring algorithm transparency and interpretability when applying optimization methods in real-world contexts.</li>
							</ul>

							<h2>Key Artefacts</h2>
							<ul>
								<li><strong>Gradient Descent Code:</strong> Implemented Python code to optimize slope (m) and intercept (b) while tracking cost reduction over iterations.</li>
								<li><strong>Observational Insights:</strong> Adjusted iteration numbers (e.g., 50, 100) and learning rates (e.g., 0.08, 0.1) to demonstrate their influence on convergence and cost minimization.</li>
								<li><strong>Discussions and Feedback:</strong> Team discussions emphasized the importance of tuning parameters for dataset variability and avoiding overfitting in model optimization.</li>
							</ul>

							<h2>Self-Reflection</h2>
							<ul>
								<li><strong>Strengths:</strong> Gained practical understanding of gradient descent mechanics and hyperparameter tuning for optimization.</li>
								<li><strong>Improvements:</strong> Enhanced skills in identifying the impact of hyperparameter values and addressing challenges in optimization for diverse datasets.</li>
							</ul>
						</div>
					</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

	</body>
</html>