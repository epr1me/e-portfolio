<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Unit 8 - Gradient Cost Function</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../../assets/css/main.css" />
		<noscript><link rel="stylesheet" href="../../assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="../../index.html" class="title">Paul Dogar</a>
				<nav>
					<ul>
						<li><a href="/e-portfolio/index.html">Back Home</a></li>
						<li><a href="/e-portfolio/modules/machine-learning.html">Machine Learning</a></li>
					</ul>
				</nav>
			</header>

		<!-- Wrapper -->
			<div id="wrapper">

				<a href="/e-portfolio/modules/machine-learning/unit-7.html" class="previous-unit button icon solid fa-angle-left">Unit 7</a>
				<a href="/e-portfolio/modules/machine-learning/unit-9.html" class="next-unit button icon solid">Unit 9 <i class="fa-angle-right"></i></a>

				<!-- Main -->
					<section id="main" class="wrapper pt-10-m">
						<div class="inner">
							<h1 class="major">Unit 8 - Gradient Cost Function</h1>
							<p>This unit focused on understanding gradient descent for optimizing linear regression models. Experimentation with iteration numbers and learning rates provided insights into cost function convergence and model optimization.</p>

							<h2>Key Learning Outcomes</h2>
							<ul>
								<li><strong>Gradient Descent Dynamics:</strong> Observed how increasing iterations reduced cost, improving model accuracy, and how learning rates impacted convergence speed and stability.</li>
								<li><strong>Applicability and Challenges:</strong> Explored hyperparameter tuning for achieving balance between speed and accuracy, and discussed generalization challenges for different datasets.</li>
								<li><strong>Ethical Considerations:</strong> Highlighted the importance of ensuring algorithm transparency and interpretability when applying optimization methods in real-world contexts.</li>
							</ul>

							<h2>Key Artefacts</h2>
							<ul>
								<li><strong>Gradient Descent Code:</strong> Implemented Python code to optimize slope (m) and intercept (b) while tracking cost reduction over iterations.</li>
								<li><strong>Observational Insights:</strong> Adjusted iteration numbers (e.g., 50, 100) and learning rates (e.g., 0.08, 0.1) to demonstrate their influence on convergence and cost minimization.</li>
								<li><strong>Discussions and Feedback:</strong> Team discussions emphasized the importance of tuning parameters for dataset variability and avoiding overfitting in model optimization.</li>
							</ul>

							<h2>Self-Reflection</h2>
							<ul>
								<li><strong>Strengths:</strong> Gained practical understanding of gradient descent mechanics and hyperparameter tuning for optimization.</li>
								<li><strong>Improvements:</strong> Enhanced skills in identifying the impact of hyperparameter values and addressing challenges in optimization for diverse datasets.</li>
							</ul>
						</div>
					</section>
				<section class="wrapper">
					<section class="inner">
						<h2>Code Showcase</h2>
						<pre>
							<code>
# Gradient descent for linear regression
import numpy as np

def gradient_descent(x, y, lr, iterations):
	m, b = 0, 0
	n = len(x)
	for _ in range(iterations):
		y_pred = m * x + b
		cost = (1/n) * sum((y - y_pred)**2)
		dm = -(2/n) * sum(x * (y - y_pred))
		db = -(2/n) * sum(y - y_pred)
		m -= lr * dm
		b -= lr * db
	return m, b, cost

x = np.array([1, 2, 3, 4])
y = np.array([2, 4, 6, 8])
m, b, final_cost = gradient_descent(x, y, lr=0.01, iterations=1000)
print("Slope:", m, "Intercept:", b, "Final Cost:", final_cost)
								</code>
	</pre>
					</section>
				</section>

			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="../../assets/js/jquery.min.js"></script>
			<script src="../../assets/js/jquery.scrollex.min.js"></script>
			<script src="../../assets/js/jquery.scrolly.min.js"></script>
			<script src="../../assets/js/browser.min.js"></script>
			<script src="../../assets/js/breakpoints.min.js"></script>
			<script src="../../assets/js/util.js"></script>
			<script src="../../assets/js/main.js"></script>

	</body>
</html>